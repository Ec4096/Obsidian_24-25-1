## 前言

培训内容包含从AI入门到科研入门，涵盖独立完成科研任务的全部内容，完成将会体验和学习到完整的科研流程。**组里涵盖方向很多，一定能找到你感兴趣的方向**。给出方向在实验室内已有良好基础和背景，且有丰富的计算资源，学长学姐可以帮忙答疑解惑。**[MARS实验室主页](https://marswhu.github.io/)****，进实验室没有时间限制，我们欢迎任何年级和任何专业的朋友，**在这里激发你的创造力，Make something interesting！Enjoy Yourself Here🤗🤗🤗

  

**本科生亮点成果：**

- **论文：**2022年以来，已有本科生发表CCF-A类论文数: 10篇（一作），大部分在大二和大三发表，包含CVPR 2024，ICML 2024， AAAI 2024，ECCV 2024，NeurIPS 2023，IJCAI 2023，ACM MM 2022 等
    
- **竞赛：**中国大学生计算机设计大赛全国一等奖、全国人工智能学会应用场景挑战赛专项一等奖等，国际计算机视觉顶会ICCV 2021赛道冠军
    
- **荣誉**：国家级大创、CCF优秀大学生、CS珞珈计科先锋、商汤奖学金、中国大学生计算机设计大赛一等奖等
    

  

本科生科研主要培养目标，你将收获以下内容：

1. 了解人工智能基础知识，读懂前沿算法代码，学会运用前沿技术解决问题
    
2. 掌握科研基本流程，锻炼严谨的科研思维，熟练运用各类科研工具和文献技巧
    
3. 探索创新性Idea并实现，在人工智能顶级会议上发表论文，参与国内外学术会议交流学习
    
4. 转换科研项目成果（**国家级大创**），斩获各种竞赛奖项，大厂名组访问实习
    

  

本次入门培训时长一个月，希望能在培训任务的指引下，带领你掌握以下内容：

- 深度学习/机器学习的基本原理：数据集，模型，训练原理等；重点掌握计算机视觉方向的常见任务；
    
- 深度学习代码的基本功：对Python、PyTorch有基本的了解，清楚Anaconda的作用，能使用Linux系统的远程服务器进行深度学习任务；能在给出GitHub仓库的情况下，根据论文内容跑通代码；能对已有代码进行改进；
    
- 科研基础：了解什么是arXiv、如何进行文献管理和阅读(Zotero)、如何做文献笔记和论文调研报告；
    
- 计算机视觉：了解计算机视觉的基本原理，了解诸如分类模型中的AlexNet、ResNet等经典卷积神经网络模型，了解以Transformer技术为基础的视觉模型ViT及其后续衍生，能够简单、基本的视觉任务训练及测试Pipeline（手写数字识别、检测等...）了解自监督学习概念，[对比学习技术](https://zhuanlan.zhihu.com/p/346686467)。了解模型剪枝的概念，[知识蒸馏技术](https://zhuanlan.zhihu.com/p/102038521)。
    
- 基本的PPT制作技能：可以考虑使用Powerpoint，或者LaTeX的Beamer包完成汇报PPT的制作
    

  

## 入门培训内容（一个月左右，主要考核是否对科研是否有兴趣，愿意投入）

1. 深度学习的基本原理和代码功底（一周左右）：
    
    1. 参考[李宏毅的机器学·习课程](https://speech.ee.ntu.edu.tw/~hylee/ml/2022-spring.php)，学习机器学习、深度学习的基本原理。
        
        1. 只需学习2/18，2/25，3/04，3/25，4/22，4/29，5/06这几日的课程，其余课程内容目前不用学习。在听课程内容时，优先听Class Material，其余的Preparation - zh和Extra Material作为补充；4/22，4/29，5/06的课程只听Preparation - zh；
            
        2. 非常建议完成布置的作业并听助教的HW讲解，强烈建议在5天之内完成这个课程。
            
    2. 你可以参考[这个CS231N专栏](https://space.bilibili.com/216720985/channel/seriesdetail?sid=1542349)，学习计算机视觉的基本原理。这个教程和第一个有一些重复，因此可以主学一个，然后快速过一遍这个专栏来查漏补缺。强烈推荐看完9、12、13和16。
        
    3. 如果对Python语言没有太多了解，可以先学习[Python基础教程](https://www.bilibili.com/video/BV1wW411Y7ai)，也可以学习https://cs231n.github.io/python-numpy-tutorial/和[Jupyter Notebook教程](https://www.dataquest.io/blog/jupyter-notebook-tutorial/)。对Linux不熟练的，学习[Linux基础教程](https://www.bilibili.com/video/BV1zx411E7KH)。对Git不熟悉的，学习[一小时Git教程](https://www.bilibili.com/video/BV1HM411377j/)。
        
    4. PyTorch的学习，建议在掌握理论知识，听完了李宏毅老师的课程的基础上，学习[PyTorch官方文档](https://pytorch.org/tutorials/beginner/basics/intro.html)给出的教程和几个示例。同时，**强烈推荐**学习[小土堆的教程](https://www.bilibili.com/video/BV1hE411t7RN/)（如果想快点上手，重点推荐这个教程）。
        
    5. 对LaTeX不熟悉的同学请学习[LaTeX基础教程](https://www.bilibili.com/video/BV1Z24y157GM/)。
        
2. 科研基础（1天左右）：通过网络搜索，了解什么是arXiv、什么是Overleaf，并初步了解如何进行文献管理和阅读、如何做文献笔记和论文调研报告，了解PapersWithCode, Google Scholar, Zotero；可以考虑学习使用Powerpoint，或者LaTeX的Beamer包完成汇报PPT的制作；了解BibTeX，知道参考文献的几种写法和格式；
    
3. 文献阅读（5-20天左右）：这部分主要是为你打下AI的研究基础。你可以选择"联邦学习"或者"多模态学习"来作为主要方向，需要阅读一些相关领域的经典论文，复现其中的一些未来常用的方法论文，并完成一份文献阅读报告，为自己未来在研究生期间的工作打下坚实的基础。
    

**如果你已经部分了解第一、二、三章所涉及到的内容，可以和导师解释，我们会安排一次面聊对其中涉及到的概念进行询问，通过者可以跳过以下内容**。

1. 阅读的内容包括但不限于：
    
    1. 第一章：概念学习。内容应当简明扼要，主要谈理解，拒绝从网上直接复制粘贴；这部分内容不超过5页；
        
        1. 联邦学习：了解联邦学习的基础概念，为什么有联邦学习这个需求。辨析联邦学习的类别、横向和纵向联邦学习。了解不同联邦学习基础的训练流程，思考以下问题：客户端本地如何更新？是否有中心服务器？服务器的作用是什么？客户端上传到服务器或者互相通信的内容（模型、特征等..）？
            
        2. 多模态学习：了解多模态的基础概念，了解不同模态的数据结构、数据集，多模态模型如何进行训练，清楚不同模态的特征如何进行融合。
            
    2. 第二章：文献阅读报告 (注意，为了避免形式主义的任务式阅读，有个小提醒：该部分的本质是为了提高你深入阅读论文的能力，提高你对方法抽象概念的理解，一些技术细节可以不用钻牛角尖，重点还是**论文背景和问题**、**论文动机和贡献解读**)。建议根据方向阅读以下给出的全部论文，并在所给文章中按个人喜好挑选**5篇**撰写阅读报告，建议选择较新的文献。
        
        1. 阅读报告的内容应包括：论文标题、作者单位、**论文背景和问题**、**论文动机和贡献解读**、方案设计详细分析、实验效果及其分析、结论、自己的思考等。撰写阅读报告时可以参考原论文、网络解读（博客、知乎专栏等）、跟进本论文的工作等。**（注意：某些较为老旧的论文原文可能在Introduction和Method部分的写作较为晦涩，经常会出现难以理解的公式、符号等，此时不用灰心，完全可以依靠****网络博客、视频等的解读****来学习这部分的内容）**这部分内容页数不限；注意，重点关注论文的**背景、问题、动机、方法部分**，实验部分适当取舍阅读，思考部分随意写一点自己真实的想法就可以（没必要上GPT来完成）
            
            1. 联邦学习：[1][3][5][6][7][8][9][10][11][13][15]
                
            2. 多模态学习：[16][17][18][22][19][20][23][24][25]
                
    3. **重要提醒****：**在你看论文的时候，可能会看到一些复杂公式不好理解，容易望而生畏，这里有一些经验建议
        
        1. 跳过公式，理解算法核心，做概念抽象的理解
            
        2. 结合知乎或者优秀帖子看，先用中文和别人嚼碎的，会好接受一点
            
        3. 结合代码跑着（断点调试），理解抓住数据input和output，数据在整个model是如何流动的（forward）（关注matrix shape）
            
    4. 第三章：代码复现报告。你需要在选择的方向的中挑选任意两个文献（参考👆🏻给出的文献），根据他们给出的GitHub仓库跑通代码，提供跑通训练pipeline到test的完整流程，并对比其是否和原文中的结果吻合。建议选择最新的文献完成代码复现。
        
    5. 第四章：考核文献阅读报告。考核方向已在下方给出。你需要任意挑选一个方向，挑选其中的两篇论文阅读、完成文献阅读报告，并做一个PPT对这两篇论文进行展示。阅读和展示考核文献时，你尤其需要思考：本文的创新点在哪里？未来是否还有能继续研究的空间？你的思考是什么？
        
    6. 第五章：未来展望（随意写一点真实感悟就好）。你需要在本文的全部内容的基础上，展现你在考核期间学习到的内容、所学内容的自我思考和未来展望，阐述自己在考核期间的心得和收获，并对考核任务的难度、平滑度进行点评和提出建议，同时阐明自己在研究生期间的学习目标和感兴趣的研究方向。
        

4. 文献调研报告的格式提示（尽量完成就好，也是规范日后学术PPT的标准）：
    
    1. **推荐**一级标题四号字体，二级标题和正文部分都用小四；
        
    2. 中文一律使用微软雅黑，英文部分使用Times New Roman字体；
        
    3. 图片、表格需要有标题。涉及到参考文献的需要设置交叉引用；
        
    4. 可以使用Word，也可以使用LaTeX，以文档美观、易于阅读为最终目标。提交的报告文件必须是PDF格式；
        
    5. 报告应图文并茂、排版美观；代码部分建议截图，**无需粘贴代码**，以美观为重。
        
5. PPT格式提示：
    
    1. PPT可以用中文或英文制作；
        
    2. 不允许设置动画，导出为PDF格式；
        
    3. 中文一律使用微软雅黑，英文部分使用Times New Roman字体；
        
    4. 图片、表格需要有标题。涉及到参考文献的需要设置交叉引用；参考文献放在本页PPT的最下方；
        
    5. 可以使用Powerpoint，也可以使用LaTeX，以文档排版美观、易于阅读为最终目标。提交的报告文件必须是PDF格式。
        

  

**友情提示：****这个不是任务，与其说是考核，不如说是我们更想让你更加深入理解科研的组成部分，明白如何进入到科研的模式中。我们不希望出现为了应付大作业式GPT的思考，你的内容可以具体或者天马行空，但不要泛泛而谈。****字数、内容都不作为好坏的标准，我们希望能够这个过程让你真正提升的，不仅知识水平，还有自学能力。**

  

- 联邦学习：了解联邦学习背景、概念[1]；了解并辨析横向联邦学习和纵向联邦学习的区别；了解Global和Personal[2]联邦学习的区别和联系；了解最基本的联邦学习baseline：FedAvg[3]，并且跑通一个[demo](https://zhuanlan.zhihu.com/p/613368320)，了解一些常用Framework(FATE、FedML)（了解一下就可以，不用跑）。了解联邦学习中的通信、隐私保护、数据结构等[4]（不是重点）。了解联邦学习中一些常用算法：[FedProx](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1812.06127)[5]、[SCAFFOLD](https://link.zhihu.com/?target=http%3A//proceedings.mlr.press/v119/karimireddy20a/karimireddy20a.pdf)[6]、[FedBN](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2102.07623)[7]，了解一些常用改进模型性能的手段：MOON[8]、FedProto[9]、kNN-Per[10]、FCCL[11]。[了解组里的通用联邦学习框架](https://github.com/WenkeHuang/MarsFL)**。**附：[联邦学习资源List](https://github.com/youngfish42/Awesome-FL#framework)。
    
    - 图学习、联邦图学习：[了解图数据结构，辨析其与图像的区别](https://zhuanlan.zhihu.com/p/606333315)。了解基本的[图神经网络GCN](https://www.zhihu.com/question/406057316/answer/2918865003)[12]、GAT、GraphSage等。了解联邦图学习的基本概念、应用[13]，了解一些Baseline：FedSage[14]、FedStar[15]。跑一个联邦图学习[demo](https://zhuanlan.zhihu.com/p/642103605)。[图学习最全资源List](https://github.com/naganandy/graph-based-deep-learning-literature/tree/master?tab=readme-ov-file) (值得follow的顶会)
        
- 多模态、跨模态学习：[了解多模态学习基本概念](https://zhuanlan.zhihu.com/p/638866979)[16]，重点关注图像、文本或者音频等不同模态是如何融合特征，如何共同训练的。十分推荐B站李沐老师的多模态相关的[论文](https://www.bilibili.com/video/BV1SL4y1s7LQ/?spm_id_from=333.337.search-card.all.click&vd_source=0b7a3cc3d3ec288abaca83b9a7e036af)[精读](https://www.bilibili.com/video/BV1Vd4y1v77v/?spm_id_from=333.788&vd_source=0b7a3cc3d3ec288abaca83b9a7e036af)。了解CLIP[17]、ALBEF[18]、BLIP[22]等多模态技术。了解意图理解的概念[19]、意图理解可以用来做什么，多模态意图理解的概念、方法[20][21]。了解[多模态学习资源List](https://github.com/pliang279/awesome-multimodal-ml)。
    
    - 跨模态行人重识别：了解行人重识别的概念、发展、挑战[23]，了解多模态、跨模态行人重识别的概念[24][25]，了解最新的相关技术[26][27][28]。
        
- 如果对其他方向感兴趣，比如：[持续学习](https://zhuanlan.zhihu.com/p/553122619)、多模态医学人工智能（**精神心理疾病、****眼科**）等，可以自行发挥，查询相关文献，**也作为可选方向之一**，下列培训内容具体文献未给出，**可以根据兴趣自行选择**，**按对等要求完成**，**考核内容已给出**。
    

  

## 联邦学习方向考核内容

重点三篇Survey文章：

Heterogeneous Federated Learning: State-of-the-art and Research Challenges. ACM Computing Surveys 2023

Federated learning for generalization, robustness, fairness: A survey and benchmark. IEEE TPAMI 2024.

Vertical Federated Learning for Effectiveness, Security, Applicability: A Survey. arXiv 2024.

Rethinking Federated Learning With Domain Shift: A Prototype View

Robust Federated Learning With Noisy and Heterogeneous Clients

Federated Graph Semantic and Structural Learning

Fedproto: Federated prototype learning across heterogeneous clients.

FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning

  

## 多模态、行人重识别方向考核内容

Transformer for Object Re-Identification: A Survey. ArXiv 2024

Deep Learning for Person Re-identification: A Survey and Outlook. IEEE TPAMI 2024.

Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval

Contextual Augmented Global Contrast for Multimodal Intent Recognition

Cross-Modality Pyramid Alignment for Visual Intention Understanding

Learnable Hierarchical Label Embedding and Grouping for Visual Intention Understanding

Towards Modality-Agnostic Person Re-identification with Descriptive Query

  

## 持续学习方向考核内容

Prototype augmentation and self-supervision for incremental learning

Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning

Heterogeneous Continual Learning

Task-Aware Information Routing from Common Representation Space in Lifelong Learning

New insights on reducing abrupt representation change in online continual learning

  

  

## 医疗AI方向考核内容

Learning From Synthetic CT Images via Test-Time Training for Liver Tumor Segmentation. IEEE TMI 2022.

Concept-based Lesion Aware Transformer for Interpretable Retinal Disease Diagnosis. IEEE TMI 2024.

Cross-Feature Interactive Tabular Data Modeling With Multiplex Graph Neural Networks. IEEE TKDE 2024.

Automatic detection of 39 fundus diseases and conditions in retinal photographs using deep neural networks

IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation

  

## 提交方式

文献阅读报告和文献汇报PPT完成后，导出为PDF格式提交到邮箱：[yemang@whu.edu.cn](mailto:yemang@whu.edu.cn)，**同时抄送到以下邮箱**。之后，导师将在阅读文献报告后邀请学生做线下文献PPT汇报。

抄送邮件：

如果选择方向为"联邦学习": 抄送至wenkehuang@whu.edu.cn，guanchengwan@whu.edu.cn各一份。

如果选择方向为"持续学习": 抄送至wuxuanshi@whu.edu.cn

如果选择方向为"医疗AI": 抄送至lihe404@whu.edu.cn

  

**Update: 23.0423 by wgc**

  

## 参考文献

[1]: Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine

learning: Concept and applications. 2019.

[2]: Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for personalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020.

[3]: Jakub Koneˇcn`y, H Brendan McMahan, Daniel Ramage, and Peter Richt´arik. Federated optimization: Distributed machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527, 2016.

[4]: Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He. A survey on federated learning systems: Vision, hype and reality for data privacy and protection. 2019.

[5]: Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2020.

[6]: Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Se[1]bastian U Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for on-device federated learning. In ICML, 2020.

[7]: Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fed{bn}: Federated learning on non-{iid} features via local batch normalization. In ICLR, 2021.

[8]: Qinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In CVPR, pages 10713–10722, 2021.

[9]: Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fedproto: Federated prototype learning across heterogeneous clients. In AAAI, 2022.

[10]: Othmane Marfoq, Giovanni Neglia, Richard Vidal, and Laetitia Kameni. Per[1]sonalized federated learning through local memorization. In ICML, pages 15070– 15092, 2022.

[11]: Wenke Huang, Mang Ye, and Bo Du. Learn from others and be yourself in heterogeneous federated learning. In CVPR, 2022.

[12]: Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.

[13]: Xingbo Fu, Binchi Zhang, Yushun Dong, Chen Chen, and Jundong Li. Federated graph machine learning: A survey of concepts, techniques, and applications. arXiv preprint arXiv:2207.11812, 2022.

[14]: Ke Zhang, Carl Yang, Xiaoxiao Li, Lichao Sun, and Siu Ming Yiu. Subgraph federated learning with missing neighbor generation. NeurIPS, 34:6671–6682, 2021.

[15]: Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang. Federated learning on non-iid graphs via structural knowledge sharing. In AAAI, 2023.

[16]: Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, and Michael Zeng. An empirical study of training end-to-end vision-and-language transformers, 2021

[17]: Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,

Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,

Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from

natural language supervision, 2021.

[18]: Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty,

Caiming Xiong, and Steven Hoi. Align before fuse: Vision and language repre-

sentation learning with momentum distillation, 2021

[19]: Intentonomy: a Dataset and Study towards Human Intent Understanding

[20]: MIntRec: A New Dataset for Multimodal Intent Recognition

[21]: MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis

[22]: BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation

[23]: Deep learning for person re-identification: A survey and outlook

[24]: CLIP-ReID: Exploiting Vision-Language Model for Image Re-Identiﬁcation without Concrete Text Labels

[25]: CMTR: Cross-modality Transformer for Visible-infrared Person Re-identiﬁcation

[26]: RGB-Infrared Cross-Modality Person Re-Identiﬁcation

[27]: Unsupervised Visible-Infrared Person Re-Identification via Progressive Graph Matching and Alternate Learning

[28]: Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval

[29]: FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning

  

  

  

Acknowledgments

Something from [Boheng Li](https://antigonerandy.github.io/), thanks for his insightful help.

  

最终解释权: 武汉大学计算机学院[叶茫教授](https://marswhu.github.io/index.html)