**LORA**是一种低资源微调大模型方法，出自论文[LoRA: Low-Rank Adaptation of Large Language Models。](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2106.09685.pdf) 使用**LORA**，训练参数仅为整体参数的万分之一、GPU显存使用量减少2/3且不会引入额外的推理耗时

[Aghajanyan](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2012.13255)的研究表明：**预训练模型拥有极小的内在维度(instrisic dimension)，即存在一个极低维度的参数，微调它和在全参数空间中微调能起到相同的效果**。


![[Pasted image 20241108180925.png]]

下·
